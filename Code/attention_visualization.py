# ============================================================
# ATTENTION VISUALIZATION CODE
# Copy to√†n b·ªô code n√†y v√†o m·ªôt cell M·ªöI trong notebook
# Ch·∫°y SAU KHI train xong
# ============================================================

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def visualize_attention_weights(model, test_loader, device, data_name='ETTh1'):
    """
    Visualize Channel Attention v√† Gated Temporal Attention weights
    
    S·ª≠ d·ª•ng: visualize_attention_weights(model, test_loader, device, 'ETTh1.csv')
    """
    model.eval()
    
    # L·∫•y 1 batch t·ª´ test
    batch_x, batch_y, batch_x_mark, batch_y_mark = next(iter(test_loader))
    batch_x = batch_x.float().to(device)
    batch_x_mark = batch_x_mark.float().to(device)
    
    # Forward pass ƒë·ªÉ t√≠nh attention weights
    with torch.no_grad():
        _ = model(batch_x, batch_x_mark)
    
    # L·∫•y weights t·ª´ layer ƒë·∫ßu ti√™n
    try:
        channel_weights = model.model[0].channel_attn.attn_weights[0].cpu().numpy()
    except:
        channel_weights = None
        print("Kh√¥ng c√≥ Channel Attention weights")
    
    try:
        gated_weights = model.model[0].gated_attn.gate_weights[0].cpu().numpy()
    except:
        gated_weights = None
        print("Kh√¥ng c√≥ Gated Temporal weights")
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle(f'Attention Weights Visualization - {data_name}', fontsize=14)
    
    # ===== 1. Channel Attention =====
    if channel_weights is not None:
        channel_names = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
        colors = ['steelblue'] * 6 + ['coral']  # OT m√†u kh√°c
        
        bars = axes[0].bar(range(len(channel_weights)), channel_weights, color=colors)
        axes[0].set_xlabel('Bi·∫øn ƒë·∫ßu v√†o', fontsize=12)
        axes[0].set_ylabel('Tr·ªçng s·ªë Attention', fontsize=12)
        axes[0].set_title('Channel Attention\n(Bi·∫øn n√†o quan tr·ªçng nh·∫•t?)', fontsize=12)
        axes[0].set_xticks(range(len(channel_weights)))
        axes[0].set_xticklabels(channel_names[:len(channel_weights)], rotation=45)
        axes[0].axhline(y=np.mean(channel_weights), color='red', linestyle='--', alpha=0.7, label='Trung b√¨nh')
        axes[0].legend()
    else:
        axes[0].text(0.5, 0.5, 'Kh√¥ng c√≥ Channel Attention', ha='center', va='center')
    
    # ===== 2. Gated Temporal Attention =====
    if gated_weights is not None:
        axes[1].plot(gated_weights, color='coral', linewidth=2)
        axes[1].fill_between(range(len(gated_weights)), gated_weights, alpha=0.3, color='coral')
        axes[1].set_xlabel('B∆∞·ªõc th·ªùi gian (Time Step)', fontsize=12)
        axes[1].set_ylabel('Gi√° tr·ªã C·ªïng (Gate)', fontsize=12)
        axes[1].set_title('Gated Temporal Attention\n(0=Gi·ªØ nguy√™n, 1=D√πng Attention)', fontsize=12)
        axes[1].set_ylim(0, 1)
        axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Ng∆∞·ª°ng 0.5')
        axes[1].legend()
    else:
        axes[1].text(0.5, 0.5, 'Kh√¥ng c√≥ Gated Temporal', ha='center', va='center')
    
    plt.tight_layout()
    plt.savefig(f'attention_{data_name.replace(".csv", "")}.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"\n‚úÖ ƒê√£ l∆∞u visualization t·∫°i: attention_{data_name.replace('.csv', '')}.png")
    
    # In th√¥ng tin chi ti·∫øt
    if channel_weights is not None:
        print("\nüìä Channel Attention Weights:")
        channel_names = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
        for i, w in enumerate(channel_weights):
            name = channel_names[i] if i < len(channel_names) else f"Channel {i}"
            print(f"   {name}: {w:.4f}")
        print(f"   ‚Üí Bi·∫øn quan tr·ªçng nh·∫•t: {channel_names[np.argmax(channel_weights)]}")
    
    if gated_weights is not None:
        print(f"\nüìä Gated Temporal Stats:")
        print(f"   Mean: {np.mean(gated_weights):.4f}")
        print(f"   Min: {np.min(gated_weights):.4f}")
        print(f"   Max: {np.max(gated_weights):.4f}")


# ============================================================
# C√ÅCH S·ª¨ D·ª§NG:
# ============================================================
# Th√™m d√≤ng n√†y v√†o cu·ªëi v√≤ng l·∫∑p benchmark, sau khi train xong:
#
# for data_name in datasets_to_run:
#     ...
#     model = train_model(...)
#     ...
#     # ===== TH√äM D√íNG N√ÄY =====
#     visualize_attention_weights(model, test_loader, device, data_name)
#     # =========================
#
# ============================================================
